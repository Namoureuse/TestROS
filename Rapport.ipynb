{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MARTINEZ Christophe p1709105  \n",
    "DAKHLI Sonia p1905223  \n",
    "# IAD RAPPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des environnements\n",
    "from world import Environment1\n",
    "from world import Environment2\n",
    "from world import Environment3\n",
    "    \n",
    "# Définition de la fonction main\n",
    "def monde1(agent, environnement, n=20):\n",
    "    outcome = 0\n",
    "    for i in range(n):\n",
    "        action = agent.action(outcome)\n",
    "        outcome = environnement.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémenter un agent rudimentaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Implémenter un agent qui apprenne à anticiper son outcome** sans connaître à priori son environnement.  \n",
    "> Et qui change d'action quand il commence à s'ennuyer (quand il effectue n fois de suite la même interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent1:\n",
    "    def __init__(self):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.count_good_pred = 0\n",
    "        self.bored_level = 4\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            if self.anticipated_outcome == outcome:\n",
    "                self.count_good_pred += 1\n",
    "        \n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = self.prev_action\n",
    "        \n",
    "        if self.count_good_pred >= self.bored_level:\n",
    "            self._action = (self.prev_action+1) % 2\n",
    "            self.count_good_pred = 0\n",
    "            \n",
    "        self.prev_action = self._action\n",
    "        \n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        if self._action in self.prev_outcomes:\n",
    "            self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "        return self._action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent1()\n",
    "e = Environment1()\n",
    "# e = Environment2()\n",
    "\n",
    "monde1(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent1()\n",
    "# e = Environment1()\n",
    "e = Environment2()\n",
    "\n",
    "monde1(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "Action: 0, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "1) Initialement nous avons définit arbitrairement que l'agent effectuerait l'action 0 et qu'il anticiperait un outcome de 0.  \n",
    "2) Il apprend donc initialement l'outcome de l'action 0 et ne peut se tromper qu'une seule fois. Ensuite il répète ce qu'il sait jusqu'à l'ennui.  \n",
    "3) Maintenant que l'agent s'ennuie, il réalise l'action 1. Comme dit précédemment, il anticipe l'outcome de l'action 1 à 0. Il apprend le résultat et répète jusqu'à l'ennui.  \n",
    "4) Puis passe à l'action 0 en répétant ce qu'il a appris précédemment, etc, etc.\n",
    "\n",
    "L'agent se trompe au maximum 1 fois pour chaque action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'agent qui préférait les interactions positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> L’agent 2 doit choisir préférentiellement les interactions qui ont une **valeur hédoniste positive**, sauf s’il s’ennuie, auquel cas il préfère faire une action différente même si elle conduit à une interaction de valeur négative.  \n",
    "> Dans la trace, on doit donc voir que l’agent répète plusieurs fois une action qui produit une interaction positive jusqu’à ce qu’il s’ennuie. Ensuite, il fait une seule fois une action différente qui produit une interaction de valeur négative. Ensuite, il fait à nouveau plusieurs fois l’action qui produit une interaction de valeur positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent2:\n",
    "    def __init__(self, valence_table):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self.valence_table = valence_table\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.count_good_pred = 0\n",
    "        self.bored_level = 4\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) +\n",
    "                  \", valence: \" + str(self.valence_table[self._action][outcome]) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            if self.anticipated_outcome == outcome:\n",
    "                self.count_good_pred += 1\n",
    "        \n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = self.prev_action\n",
    "        \n",
    "        best_valence = -1\n",
    "        for a,o in self.prev_outcomes.items():\n",
    "            valence = self.valence_table[a][o]\n",
    "            if valence > best_valence:\n",
    "                best_valence = valence\n",
    "                self._action = a      \n",
    "        if self._action != self.prev_action:\n",
    "            self.count_good_pred = 0\n",
    "        \n",
    "        if self.count_good_pred >= self.bored_level:\n",
    "            self._action = (self.prev_action+1) % 2\n",
    "            self.count_good_pred = 0\n",
    "            \n",
    "        self.prev_action = self._action\n",
    "        \n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        if self._action in self.prev_outcomes:\n",
    "            self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "        return self._action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valences = [[-1, 1], [-1, 1]]\n",
    "# valences = [[1, -1], [1, -1]]\n",
    "\n",
    "a = Agent2(valences)\n",
    "\n",
    "e = Environment1()\n",
    "# e = Environment2()\n",
    "\n",
    "monde1(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 1, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valences = [[-1, 1], [-1, 1]]\n",
    "# valences = [[1, -1], [1, -1]]\n",
    "\n",
    "a = Agent2(valences)\n",
    "\n",
    "# e = Environment1()\n",
    "e = Environment2()\n",
    "\n",
    "monde1(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "Action: 0, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "\n",
    "L'agent2 se comporte de la même manière que l'agent1 en ce qui concerne l'anticipation des outcomes de ses actions.  \n",
    "En plus de cela, l'agent cherche à maximiser sa valence pour déterminer quelle action effectuer.  \n",
    "\n",
    "1) Initialement, l'agent effectue la seule action qu'il connait peu importe la valence associée, jusqu'à l'ennui.  \n",
    "2) Ensuite, l'agent passe à l'autre action possible.\n",
    "3) a) Si cette action est associée à une valence positive, il continue jusqu'à l'ennui.  \n",
    "b) Sinon il rechange d'action pour une meilleure valence.\n",
    "4) Il répète ensuite 4 fois les actions avec une meilleure valence et 1 fois les actions avec une moins bonne valence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les comportements des agents sont bien associés à de l'IA développementale car les agents commencent par effectuer une action dans leur environnement. Ce dernier répond par un outcome que l'agent enregistre dans sa mémoire ce qui lui permet d'apprendre à prédire ses prochaines interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'agent qui pilotait une tortue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtlepy_enacter import TurtlePyEnacter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent3:\n",
    "    def __init__(self, valence_table, bored_level=4):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self.valence_table = valence_table\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.nb_actions = 3\n",
    "        \n",
    "        self.count_good_pred = 0\n",
    "        self.bored_level = bored_level\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) +\n",
    "                  \", valence: \" + str(self.valence_table[self._action][outcome]) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            if self.anticipated_outcome == outcome:\n",
    "                self.count_good_pred += 1\n",
    "        \n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = self.prev_action\n",
    "        \n",
    "        best_valence = -1\n",
    "        for a,o in self.prev_outcomes.items():\n",
    "            valence = self.valence_table[a][o]\n",
    "            if valence > best_valence:\n",
    "                best_valence = valence\n",
    "                self._action = a      \n",
    "        if self._action != self.prev_action:\n",
    "            self.count_good_pred = 0\n",
    "        \n",
    "        if self.count_good_pred >= self.bored_level:\n",
    "            self._action = (self.prev_action+1) % self.nb_actions\n",
    "            self.count_good_pred = 0\n",
    "            \n",
    "        self.prev_action = self._action\n",
    "        \n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        if self._action in self.prev_outcomes:\n",
    "            self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "        return self._action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TurtlePyEnacter()\n",
    "\n",
    "# valences = [[1,-1], [1,-1], [1,-1]]\n",
    "valences = [[-1,-1], [1,-1], [1,-1]]\n",
    "# valences = [[1,-1], [-1,-1], [-1,-1]]\n",
    "\n",
    "agent3 = Agent3(valences, 4)\n",
    "outcome = 0\n",
    "\n",
    "for i in range(500):\n",
    "    action = agent3.action(outcome)\n",
    "    outcome = x.outcome(action)\n",
    "    \n",
    "x.screen.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Agent3 avec bored_level=1](img/agent3_bored_lvl_1.png)\n",
    "*bored_level = 1*\n",
    "![Agent3 avec bored_level=4](img/agent3_bored_lvl_4.png)\n",
    "*bored_level = 4*\n",
    "![Agent3 avec bored_level=10](img/agent3_bored_lvl_10.png)\n",
    "*bored_level = 10*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "\n",
    "**En ne modifiant que le bored_level**  \n",
    "La première chose que l'on remarque est que l'agent finit par se coincer dans une succession d'actions se répétant en boucle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'agent qui adaptait ses actions au contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources import Interaction\n",
    "\n",
    "class Agent4:\n",
    "    def __init__(self, valence_table, bored_level=4):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self.valence_table = valence_table\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.nb_actions = len(valence_table)\n",
    "        \n",
    "        self.interactions = {}\n",
    "        self.interaction_tm1 = None\n",
    "        self.interaction_tm2 = None\n",
    "        \n",
    "        self.count_bored = 0\n",
    "        self.bored_level = bored_level\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "        \n",
    "        # Réinitialisation des interactions enregistrées\n",
    "        Interaction.interaction_list = []\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) +\n",
    "                  \", valence: \" + str(self.valence_table[self._action][outcome]) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        self.interaction_tm2 = self.interaction_tm1\n",
    "        \n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            self.interaction_tm1 = Interaction.create_or_retrieve(self._action, outcome, self.valence_table[self._action][outcome])\n",
    "        \n",
    "        if self.interaction_tm2 is not None:\n",
    "            self.interactions[self.interaction_tm2] = self.interaction_tm1\n",
    "        \n",
    "        # Decision and anticipation\n",
    "        self._action = 0\n",
    "        self.anticipated_outcome = None\n",
    "        next_iteration = None\n",
    "        \n",
    "        # Prédiction si cela est possible\n",
    "        if self.interaction_tm1 is not None and self.interaction_tm1 in self.interactions:\n",
    "            next_iteration = self.interactions[self.interaction_tm1]\n",
    "            \n",
    "        if next_iteration is not None:\n",
    "            next_outcome = next_iteration.outcome\n",
    "            # Si l'action prédite à une valence supposée > 0 et que l'agent ne s'ennui pas\n",
    "            if next_iteration.valence > 0 and self.count_bored < (self.bored_level-1):\n",
    "                self._action = next_iteration.action\n",
    "                self.anticipated_outcome = next_outcome\n",
    "            else:\n",
    "                # Sinon on essaye une autre action\n",
    "                self._action = (next_iteration.action + 1) % self.nb_actions\n",
    "                # On essaye de prédire l'outcome si possible\n",
    "                if self._action in self.prev_outcomes:\n",
    "                    self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "            \n",
    "            # Ennui\n",
    "            if self._action == self.prev_action and self.anticipated_outcome == outcome:\n",
    "                self.count_bored += 1\n",
    "            else:\n",
    "                self.count_bored = 0\n",
    "        \n",
    "        self.prev_action = self._action\n",
    "        return self._action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valences = [[-1, 1], [-1, 1]]\n",
    "# valences = [[1, -1], [1, -1]]\n",
    "\n",
    "a4 = Agent4(valences)\n",
    "\n",
    "# e = Environment1()\n",
    "# e = Environment2()\n",
    "e = Environment3()\n",
    "\n",
    "monde1(a4,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Anticipation: None, Outcome: 0, Satisfaction: (anticipation: False, valence: 1)\n",
      "Action: 0, Anticipation: None, Outcome: 0, Satisfaction: (anticipation: False, valence: 1)\n",
      "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
      "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
      "Action: 0, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False, valence: -1)\n",
      "Action: 0, Anticipation: None, Outcome: 1, Satisfaction: (anticipation: False, valence: -1)\n",
      "Action: 1, Anticipation: None, Outcome: 1, Satisfaction: (anticipation: False, valence: -1)\n",
      "Action: 0, Anticipation: None, Outcome: 1, Satisfaction: (anticipation: False, valence: -1)\n",
      "Action: 2, Anticipation: None, Outcome: 1, Satisfaction: (anticipation: False, valence: -1)\n",
      "Action: 0, Anticipation: None, Outcome: 1, Satisfaction: (anticipation: False, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 2, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: -1)\n"
     ]
    },
    {
     "ename": "Terminator",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminator\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/stouf/Documents/Master/TestROS/Rapport.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stouf/Documents/Master/TestROS/Rapport.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m200\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stouf/Documents/Master/TestROS/Rapport.ipynb#X40sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     action \u001b[39m=\u001b[39m a4\u001b[39m.\u001b[39maction(outcome)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/stouf/Documents/Master/TestROS/Rapport.ipynb#X40sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     outcome \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49moutcome(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stouf/Documents/Master/TestROS/Rapport.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x\u001b[39m.\u001b[39mscreen\u001b[39m.\u001b[39mmainloop()\n",
      "File \u001b[0;32m~/Documents/Master/TestROS/turtlepy_enacter.py:51\u001b[0m, in \u001b[0;36mTurtlePyEnacter.outcome\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39melif\u001b[39;00m action \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[39m# rotate left\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mturtle\u001b[39m.\u001b[39mspeed(\u001b[39m10\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mturtle\u001b[39m.\u001b[39;49mleft(\u001b[39m4\u001b[39;49m)\n\u001b[1;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mturtle\u001b[39m.\u001b[39mforward(\u001b[39m2\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[39melif\u001b[39;00m action \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m     54\u001b[0m     \u001b[39m# rotate right\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/turtle.py:1700\u001b[0m, in \u001b[0;36mTNavigator.left\u001b[0;34m(self, angle)\u001b[0m\n\u001b[1;32m   1681\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mleft\u001b[39m(\u001b[39mself\u001b[39m, angle):\n\u001b[1;32m   1682\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Turn turtle left by angle units.\u001b[39;00m\n\u001b[1;32m   1683\u001b[0m \n\u001b[1;32m   1684\u001b[0m \u001b[39m    Aliases: left | lt\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[39m    67.0\u001b[39;00m\n\u001b[1;32m   1699\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1700\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rotate(angle)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/turtle.py:3293\u001b[0m, in \u001b[0;36mRawTurtle._rotate\u001b[0;34m(self, angle)\u001b[0m\n\u001b[1;32m   3291\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(steps):\n\u001b[1;32m   3292\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_orient \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_orient\u001b[39m.\u001b[39mrotate(delta)\n\u001b[0;32m-> 3293\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update()\n\u001b[1;32m   3294\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_orient \u001b[39m=\u001b[39m neworient\n\u001b[1;32m   3295\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/turtle.py:2661\u001b[0m, in \u001b[0;36mRawTurtle._update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2659\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   2660\u001b[0m \u001b[39melif\u001b[39;00m screen\u001b[39m.\u001b[39m_tracing \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2661\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_data()\n\u001b[1;32m   2662\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_drawturtle()\n\u001b[1;32m   2663\u001b[0m     screen\u001b[39m.\u001b[39m_update()                  \u001b[39m# TurtleScreenBase\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/turtle.py:2647\u001b[0m, in \u001b[0;36mRawTurtle._update_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2646\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 2647\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscreen\u001b[39m.\u001b[39;49m_incrementudc()\n\u001b[1;32m   2648\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscreen\u001b[39m.\u001b[39m_updatecounter \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2649\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/turtle.py:1293\u001b[0m, in \u001b[0;36mTurtleScreen._incrementudc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m TurtleScreen\u001b[39m.\u001b[39m_RUNNING:\n\u001b[1;32m   1292\u001b[0m     TurtleScreen\u001b[39m.\u001b[39m_RUNNING \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 1293\u001b[0m     \u001b[39mraise\u001b[39;00m Terminator\n\u001b[1;32m   1294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tracing \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1295\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_updatecounter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mTerminator\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Trutle\n",
    "\n",
    "x = TurtlePyEnacter()\n",
    "\n",
    "valences = [[1,-1], [1,-1], [1,-1]]\n",
    "# valences = [[-1,-1], [1,-1], [1,-1]]\n",
    "# valences = [[1,-1], [-1,-1], [-1,-1]]\n",
    "\n",
    "a4 = Agent4(valences, 4)\n",
    "outcome = 0\n",
    "\n",
    "for i in range(200):\n",
    "    action = a4.action(outcome)\n",
    "    outcome = x.outcome(action)\n",
    "    \n",
    "x.screen.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
