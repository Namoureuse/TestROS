{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MARTINEZ Christophe p1709105  \n",
    "DAKHLI Sonia p1905223  \n",
    "# IAD RAPPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des environnements\n",
    "from world import Environment1\n",
    "from world import Environment2\n",
    "from world import Environment3\n",
    "    \n",
    "# Définition de la fonction main\n",
    "def monde1(agent, environnement, n=20):\n",
    "    outcome = 0\n",
    "    for i in range(n):\n",
    "        action = agent.action(outcome)\n",
    "        outcome = environnement.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémenter un agent rudimentaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Implémenter un agent qui apprenne à anticiper son outcome** sans connaître à priori son environnement.  \n",
    "> Et qui change d'action quand il commence à s'ennuyer (quand il effectue n fois de suite la même interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent1:\n",
    "    def __init__(self):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.count_good_pred = 0\n",
    "        self.bored_level = 4\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            if self.anticipated_outcome == outcome:\n",
    "                self.count_good_pred += 1\n",
    "        \n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = self.prev_action\n",
    "        \n",
    "        if self.count_good_pred >= self.bored_level:\n",
    "            self._action = (self.prev_action+1) % 2\n",
    "            self.count_good_pred = 0\n",
    "            \n",
    "        self.prev_action = self._action\n",
    "        \n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        if self._action in self.prev_outcomes:\n",
    "            self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "        return self._action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent1()\n",
    "e = Environment1()\n",
    "# e = Environment2()\n",
    "\n",
    "monde1(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent1()\n",
    "# e = Environment1()\n",
    "e = Environment2()\n",
    "\n",
    "monde1(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "Action: 0, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "1) Initialement nous avons définit arbitrairement que l'agent effectuerait l'action 0 et qu'il anticiperait un outcome de 0.  \n",
    "2) Il apprend donc initialement l'outcome de l'action 0 et ne peut se tromper qu'une seule fois. Ensuite il répète ce qu'il sait jusqu'à l'ennui.  \n",
    "3) Maintenant que l'agent s'ennuie, il réalise l'action 1. Comme dit précédemment, il anticipe l'outcome de l'action 1 à 0. Il apprend le résultat et répète jusqu'à l'ennui.  \n",
    "4) Puis passe à l'action 0 en répétant ce qu'il a appris précédemment, etc, etc.\n",
    "\n",
    "L'agent se trompe au maximum 1 fois pour chaque action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'agent qui préférait les interactions positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> L’agent 2 doit choisir préférentiellement les interactions qui ont une **valeur hédoniste positive**, sauf s’il s’ennuie, auquel cas il préfère faire une action différente même si elle conduit à une interaction de valeur négative.  \n",
    "> Dans la trace, on doit donc voir que l’agent répète plusieurs fois une action qui produit une interaction positive jusqu’à ce qu’il s’ennuie. Ensuite, il fait une seule fois une action différente qui produit une interaction de valeur négative. Ensuite, il fait à nouveau plusieurs fois l’action qui produit une interaction de valeur positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent2:\n",
    "    def __init__(self, valence_table):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self.valence_table = valence_table\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.count_good_pred = 0\n",
    "        self.bored_level = 4\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) +\n",
    "                  \", valence: \" + str(self.valence_table[self._action][outcome]) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            if self.anticipated_outcome == outcome:\n",
    "                self.count_good_pred += 1\n",
    "        \n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = self.prev_action\n",
    "        \n",
    "        best_valence = -1\n",
    "        for a,o in self.prev_outcomes.items():\n",
    "            valence = self.valence_table[a][o]\n",
    "            if valence > best_valence:\n",
    "                best_valence = valence\n",
    "                self._action = a      \n",
    "        if self._action != self.prev_action:\n",
    "            self.count_good_pred = 0\n",
    "        \n",
    "        if self.count_good_pred >= self.bored_level:\n",
    "            self._action = (self.prev_action+1) % 2\n",
    "            self.count_good_pred = 0\n",
    "            \n",
    "        self.prev_action = self._action\n",
    "        \n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        if self._action in self.prev_outcomes:\n",
    "            self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "        return self._action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valences = [[-1, 1], [-1, 1]]\n",
    "# valences = [[1, -1], [1, -1]]\n",
    "\n",
    "a = Agent2(valences)\n",
    "\n",
    "e = Environment1()\n",
    "# e = Environment2()\n",
    "\n",
    "monde1(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 1, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valences = [[-1, 1], [-1, 1]]\n",
    "# valences = [[1, -1], [1, -1]]\n",
    "\n",
    "a = Agent2(valences)\n",
    "\n",
    "# e = Environment1()\n",
    "e = Environment2()\n",
    "\n",
    "monde1(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "Action: 0, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "\n",
    "L'agent2 se comporte de la même manière que l'agent1 en ce qui concerne l'anticipation des outcomes de ses actions.  \n",
    "En plus de cela, l'agent cherche à maximiser sa valence pour déterminer quelle action effectuer.  \n",
    "\n",
    "1) Initialement, l'agent effectue la seule action qu'il connait peu importe la valence associée, jusqu'à l'ennui.  \n",
    "2) Ensuite, l'agent passe à l'autre action possible.\n",
    "3) a) Si cette action est associée à une valence positive, il continue jusqu'à l'ennui.  \n",
    "b) Sinon il rechange d'action pour une meilleure valence.\n",
    "4) Il répète ensuite 4 fois les actions avec une meilleure valence et 1 fois les actions avec une moins bonne valence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les comportements des agents sont bien associés à de l'IA développementale car les agents commencent par effectuer une action dans leur environnement. Ce dernier répond par un outcome que l'agent enregistre dans sa mémoire ce qui lui permet d'apprendre à prédire ses prochaines interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'agent qui pilotait une tortue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtlepy_enacter import TurtlePyEnacter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent3:\n",
    "    def __init__(self, valence_table, bored_level=4):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self.valence_table = valence_table\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.nb_actions = 3\n",
    "        \n",
    "        self.count_good_pred = 0\n",
    "        self.bored_level = bored_level\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) +\n",
    "                  \", valence: \" + str(self.valence_table[self._action][outcome]) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            if self.anticipated_outcome == outcome:\n",
    "                self.count_good_pred += 1\n",
    "        \n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = self.prev_action\n",
    "        \n",
    "        best_valence = -1\n",
    "        for a,o in self.prev_outcomes.items():\n",
    "            valence = self.valence_table[a][o]\n",
    "            if valence > best_valence:\n",
    "                best_valence = valence\n",
    "                self._action = a      \n",
    "        if self._action != self.prev_action:\n",
    "            self.count_good_pred = 0\n",
    "        \n",
    "        if self.count_good_pred >= self.bored_level:\n",
    "            self._action = (self.prev_action+1) % self.nb_actions\n",
    "            self.count_good_pred = 0\n",
    "            \n",
    "        self.prev_action = self._action\n",
    "        \n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        if self._action in self.prev_outcomes:\n",
    "            self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "        return self._action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
      "Action: 1, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False, valence: -1)\n"
     ]
    },
    {
     "ename": "Terminator",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminator\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m):\n\u001b[1;32m     11\u001b[0m     action \u001b[39m=\u001b[39m agent3\u001b[39m.\u001b[39maction(outcome)\n\u001b[0;32m---> 12\u001b[0m     outcome \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49moutcome(action)\n\u001b[1;32m     14\u001b[0m x\u001b[39m.\u001b[39mscreen\u001b[39m.\u001b[39mmainloop()\n",
      "File \u001b[0;32m~/Documents/Master/TestROS/turtlepy_enacter.py:50\u001b[0m, in \u001b[0;36mTurtlePyEnacter.outcome\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mturtle\u001b[39m.\u001b[39mforward(\u001b[39m5\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[39melif\u001b[39;00m action \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[39m# rotate left\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mturtle\u001b[39m.\u001b[39;49mspeed(\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mturtle\u001b[39m.\u001b[39mleft(\u001b[39m4\u001b[39m)\n\u001b[1;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mturtle\u001b[39m.\u001b[39mforward(\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib64/python3.11/turtle.py:2175\u001b[0m, in \u001b[0;36mTPen.speed\u001b[0;34m(self, speed)\u001b[0m\n\u001b[1;32m   2173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2174\u001b[0m     speed \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 2175\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpen(speed\u001b[39m=\u001b[39;49mspeed)\n",
      "File \u001b[0;32m/usr/lib64/python3.11/turtle.py:2460\u001b[0m, in \u001b[0;36mTPen.pen\u001b[0;34m(self, pen, **pendict)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     sa, ca \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tilt), math\u001b[39m.\u001b[39mcos(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tilt)\n\u001b[1;32m   2458\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shapetrafo \u001b[39m=\u001b[39m ( scx\u001b[39m*\u001b[39mca, scy\u001b[39m*\u001b[39m(shf\u001b[39m*\u001b[39mca \u001b[39m+\u001b[39m sa),\n\u001b[1;32m   2459\u001b[0m                         \u001b[39m-\u001b[39mscx\u001b[39m*\u001b[39msa, scy\u001b[39m*\u001b[39m(ca \u001b[39m-\u001b[39m shf\u001b[39m*\u001b[39msa))\n\u001b[0;32m-> 2460\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update()\n",
      "File \u001b[0;32m/usr/lib64/python3.11/turtle.py:2661\u001b[0m, in \u001b[0;36mRawTurtle._update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2659\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   2660\u001b[0m \u001b[39melif\u001b[39;00m screen\u001b[39m.\u001b[39m_tracing \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2661\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_data()\n\u001b[1;32m   2662\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_drawturtle()\n\u001b[1;32m   2663\u001b[0m     screen\u001b[39m.\u001b[39m_update()                  \u001b[39m# TurtleScreenBase\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.11/turtle.py:2647\u001b[0m, in \u001b[0;36mRawTurtle._update_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2646\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 2647\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscreen\u001b[39m.\u001b[39;49m_incrementudc()\n\u001b[1;32m   2648\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscreen\u001b[39m.\u001b[39m_updatecounter \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2649\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.11/turtle.py:1293\u001b[0m, in \u001b[0;36mTurtleScreen._incrementudc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m TurtleScreen\u001b[39m.\u001b[39m_RUNNING:\n\u001b[1;32m   1292\u001b[0m     TurtleScreen\u001b[39m.\u001b[39m_RUNNING \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 1293\u001b[0m     \u001b[39mraise\u001b[39;00m Terminator\n\u001b[1;32m   1294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tracing \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1295\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_updatecounter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mTerminator\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = TurtlePyEnacter()\n",
    "\n",
    "# valences = [[1,-1], [1,-1], [1,-1]]\n",
    "valences = [[-1,-1], [1,-1], [1,-1]]\n",
    "# valences = [[1,-1], [-1,-1], [-1,-1]]\n",
    "\n",
    "agent3 = Agent3(valences, 4)\n",
    "outcome = 0\n",
    "\n",
    "for i in range(500):\n",
    "    action = agent3.action(outcome)\n",
    "    outcome = x.outcome(action)\n",
    "    \n",
    "x.screen.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Agent3 avec bored_level=1](img/agent3_bored_lvl_1.png)\n",
    "*bored_level = 1*\n",
    "![Agent3 avec bored_level=4](img/agent3_bored_lvl_4.png)\n",
    "*bored_level = 4*\n",
    "![Agent3 avec bored_level=10](img/agent3_bored_lvl_10.png)\n",
    "*bored_level = 10*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "\n",
    "**En ne modifiant que le bored_level**  \n",
    "La première chose que l'on remarque est que l'agent finit par se coincer dans une succession d'actions se répétant en boucle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'agent qui adaptait ses actions au contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources import Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent4:\n",
    "    def __init__(self, valence_table, bored_level=4):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self.valence_table = valence_table\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.nb_actions = 3\n",
    "        \n",
    "        self.count_good_pred = 0\n",
    "        self.bored_level = bored_level\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) +\n",
    "                  \", valence: \" + str(self.valence_table[self._action][outcome]) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        interaction = None\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            interaction = Interaction.create_or_retrieve(self._action, outcome, self.valence_table[self._action][outcome])\n",
    "            if self.anticipated_outcome == outcome:\n",
    "                self.count_good_pred += 1\n",
    "        \n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = self.prev_action\n",
    "        \n",
    "        best_valence = -1\n",
    "        for a,o in self.prev_outcomes.items():\n",
    "            valence = self.valence_table[a][o]\n",
    "            if valence > best_valence:\n",
    "                best_valence = valence\n",
    "                self._action = a      \n",
    "        if self._action != self.prev_action:\n",
    "            self.count_good_pred = 0\n",
    "        \n",
    "        if self.count_good_pred >= self.bored_level:\n",
    "            self._action = (self.prev_action+1) % self.nb_actions\n",
    "\n",
    "            self.count_good_pred = 0\n",
    "            \n",
    "        self.prev_action = self._action\n",
    "        \n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        if self._action in self.prev_outcomes:\n",
    "            self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "        return self._action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
