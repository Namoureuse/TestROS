{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MARTINEZ Christophe p1709105  \n",
    "DAKHLI Sonia p1905223  \n",
    "# IAD RAPPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des environnements\n",
    "class Environment1:\n",
    "    \"\"\" In Environment 1, action 0 yields outcome 0, action 1 yields outcome 1 \"\"\"\n",
    "    def outcome(self, action):\n",
    "        # return int(input(\"entre 0 1 ou 2\"))\n",
    "        if action == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "class Environment2:\n",
    "    \"\"\" In Environment 2, action 0 yields outcome 1, action 1 yields outcome 0 \"\"\"\n",
    "    def outcome(self, action):\n",
    "        if action == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "class Environment3:\n",
    "    \"\"\" Environment 3 yields outcome 1 only when the agent alternates actions 0 and 1 \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializing Environment3 \"\"\"\n",
    "        self.previous_action = 0\n",
    "\n",
    "    def outcome(self, action):\n",
    "        _outcome = 1\n",
    "        if action == self.previous_action:\n",
    "            _outcome = 0\n",
    "        self.previous_action = action\n",
    "        return _outcome\n",
    "    \n",
    "# Définition de la fonction main\n",
    "def main(agent, environnement, n=20):\n",
    "    outcome = 0\n",
    "    for i in range(n):\n",
    "        action = agent.action(outcome)\n",
    "        outcome = environnement.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémenter un agent rudimentaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Implémenter un agent qui apprenne à anticiper son outcome** sans connaître à priori son environnement.  \n",
    "> Et qui change d'action quand il commence à s'ennuyer (quand il effectue n fois de suite la même interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent1:\n",
    "    def __init__(self):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.count_good_pred = 0\n",
    "        self.bored_level = 4\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            if self.anticipated_outcome == outcome:\n",
    "                self.count_good_pred += 1\n",
    "        \n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = self.prev_action\n",
    "        \n",
    "        if self.count_good_pred >= self.bored_level:\n",
    "            self._action = (self.prev_action+1) % 2\n",
    "            self.count_good_pred = 0\n",
    "            \n",
    "        self.prev_action = self._action\n",
    "        \n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        if self._action in self.prev_outcomes:\n",
    "            self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "        return self._action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent1()\n",
    "e = Environment1()\n",
    "# e = Environment2()\n",
    "\n",
    "main(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent1()\n",
    "# e = Environment1()\n",
    "e = Environment2()\n",
    "\n",
    "main(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "Action: 0, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "1) Initialement nous avons définit arbitrairement que l'agent effectuerait l'action 0 et qu'il anticiperait un outcome de 0.  \n",
    "2) Il apprend donc initialement l'outcome de l'action 0 et ne peut se tromper qu'une seule fois. Ensuite il répète ce qu'il sait jusqu'à l'ennui.  \n",
    "3) Maintenant que l'agent s'ennuie, il réalise l'action 1. Comme dit précédemment, il anticipe l'outcome de l'action 1 à 0. Il apprend le résultat et répète jusqu'à l'ennui.  \n",
    "4) Puis passe à l'action 0 en répétant ce qu'il a appris précédemment, etc, etc.\n",
    "\n",
    "L'agent se trompe au maximum 1 fois pour chaque action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'agent qui préférait les interactions positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> L’agent 2 doit choisir préférentiellement les interactions qui ont une **valeur hédoniste positive**, sauf s’il s’ennuie, auquel cas il préfère faire une action différente même si elle conduit à une interaction de valeur négative.  \n",
    "> Dans la trace, on doit donc voir que l’agent répète plusieurs fois une action qui produit une interaction positive jusqu’à ce qu’il s’ennuie. Ensuite, il fait une seule fois une action différente qui produit une interaction de valeur négative. Ensuite, il fait à nouveau plusieurs fois l’action qui produit une interaction de valeur positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent2:\n",
    "    def __init__(self, valence_table):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self.valence_table = valence_table\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.count_good_pred = 0\n",
    "        self.bored_level = 4\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) +\n",
    "                  \", valence: \" + str(self.valence_table[self._action][outcome]) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            if self.anticipated_outcome == outcome:\n",
    "                self.count_good_pred += 1\n",
    "        \n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = self.prev_action\n",
    "        \n",
    "        best_valence = -1\n",
    "        for a,o in self.prev_outcomes.items():\n",
    "            valence = self.valence_table[a][o]\n",
    "            if valence > best_valence:\n",
    "                best_valence = valence\n",
    "                self._action = a      \n",
    "        if self._action != self.prev_action:\n",
    "            self.count_good_pred = 0\n",
    "        \n",
    "        if self.count_good_pred >= self.bored_level:\n",
    "            self._action = (self.prev_action+1) % 2\n",
    "            self.count_good_pred = 0\n",
    "            \n",
    "        self.prev_action = self._action\n",
    "        \n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        if self._action in self.prev_outcomes:\n",
    "            self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "        return self._action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valences = [[-1, 1], [-1, 1]]\n",
    "# valences = [[1, -1], [1, -1]]\n",
    "\n",
    "a = Agent2(valences)\n",
    "\n",
    "e = Environment1()\n",
    "# e = Environment2()\n",
    "\n",
    "main(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 1, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valences = [[-1, 1], [-1, 1]]\n",
    "# valences = [[1, -1], [1, -1]]\n",
    "\n",
    "a = Agent2(valences)\n",
    "\n",
    "# e = Environment1()\n",
    "e = Environment2()\n",
    "\n",
    "main(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "Action: 0, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "\n",
    "L'agent2 se comporte de la même manière que l'agent1 en ce qui concerne l'anticipation des outcomes de ses actions.  \n",
    "En plus de cela, l'agent cherche à maximiser sa valence pour déterminer quelle action effectuer.  \n",
    "\n",
    "1) Initialement, l'agent effectue la seule action qu'il connait peu importe la valence associée, jusqu'à l'ennui.  \n",
    "2) Ensuite, l'agent passe à l'autre action possible.\n",
    "3) a) Si cette action est associée à une valence positive, il continue jusqu'à l'ennui.  \n",
    "b) Sinon il rechange d'action pour une meilleure valence.\n",
    "4) Il répète ensuite 4 fois les actions avec une meilleure valence et 1 fois les actions avec une moins bonne valence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les comportements des agents sont bien associés à de l'IA développementale car les agents commencent par effectuer une action dans leur environnement. Ce dernier répond par un outcome que l'agent enregistre dans sa mémoire ce qui lui permet d'apprendre à prédire ses prochaines interactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
