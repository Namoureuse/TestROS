{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MARTINEZ Christophe p1709105  \n",
    "DAKHLI Sonia p1905223  \n",
    "# IAD RAPPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des environnements\n",
    "from world import Environment1\n",
    "from world import Environment2\n",
    "from world import Environment3\n",
    "    \n",
    "# Définition de la fonction main\n",
    "def monde1(agent, environnement, n=20):\n",
    "    outcome = 0\n",
    "    for i in range(n):\n",
    "        action = agent.action(outcome)\n",
    "        outcome = environnement.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémenter un agent rudimentaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Implémenter un agent qui apprenne à anticiper son outcome** sans connaître à priori son environnement.  \n",
    "> Et qui change d'action quand il commence à s'ennuyer (quand il effectue n fois de suite la même interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent1:\n",
    "    def __init__(self):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.count_good_pred = 0\n",
    "        self.bored_level = 4\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            if self.anticipated_outcome == outcome:\n",
    "                self.count_good_pred += 1\n",
    "        \n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = self.prev_action\n",
    "        \n",
    "        if self.count_good_pred >= self.bored_level:\n",
    "            self._action = (self.prev_action+1) % 2\n",
    "            self.count_good_pred = 0\n",
    "            \n",
    "        self.prev_action = self._action\n",
    "        \n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        if self._action in self.prev_outcomes:\n",
    "            self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "        return self._action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent1()\n",
    "e = Environment1()\n",
    "# e = Environment2()\n",
    "\n",
    "monde1(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "e = Environment1()\n",
    "\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent1()\n",
    "# e = Environment1()\n",
    "e = Environment2()\n",
    "\n",
    "monde1(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "e = Environment2()\n",
    "\n",
    "Action: 0, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "1) Initialement nous avons défini arbitrairement que l'agent effectuerait l'action 0 et qu'il anticiperait un outcome de 0.  \n",
    "2) Il apprend donc initialement l'outcome de l'action 0 et ne peut se tromper qu'une seule fois. Ensuite il répète ce qu'il sait jusqu'à l'ennui.  \n",
    "3) Maintenant que l'agent s'ennuie, il réalise l'action 1. Comme dit précédemment, il anticipe l'outcome de l'action 1 à 0. Il apprend le résultat et répète jusqu'à l'ennui.  \n",
    "4) Puis passe à l'action 0 en répétant ce qu'il a appris précédemment, etc, etc.\n",
    "\n",
    "L'agent se trompe au maximum 1 fois pour chaque action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'agent qui préférait les interactions positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> L’agent 2 doit choisir préférentiellement les interactions qui ont une **valeur hédoniste positive**, sauf s’il s’ennuie, auquel cas il préfère faire une action différente même si elle conduit à une interaction de valeur négative.  \n",
    "> Dans la trace, on doit donc voir que l’agent répète plusieurs fois une action qui produit une interaction positive jusqu’à ce qu’il s’ennuie. Ensuite, il fait une seule fois une action différente qui produit une interaction de valeur négative. Ensuite, il fait à nouveau plusieurs fois l’action qui produit une interaction de valeur positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent2:\n",
    "    def __init__(self, valence_table):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self.valence_table = valence_table\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.count_good_pred = 0\n",
    "        self.bored_level = 4\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) +\n",
    "                  \", valence: \" + str(self.valence_table[self._action][outcome]) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            if self.anticipated_outcome == outcome:\n",
    "                self.count_good_pred += 1\n",
    "        \n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = self.prev_action\n",
    "        \n",
    "        best_valence = -1\n",
    "        for a,o in self.prev_outcomes.items():\n",
    "            valence = self.valence_table[a][o]\n",
    "            if valence > best_valence:\n",
    "                best_valence = valence\n",
    "                self._action = a      \n",
    "        if self._action != self.prev_action:\n",
    "            self.count_good_pred = 0\n",
    "        \n",
    "        if self.count_good_pred >= self.bored_level:\n",
    "            self._action = (self.prev_action+1) % 2\n",
    "            self.count_good_pred = 0\n",
    "            \n",
    "        self.prev_action = self._action\n",
    "        \n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        if self._action in self.prev_outcomes:\n",
    "            self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "        return self._action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valences = [[-1, 1], [-1, 1]]\n",
    "# valences = [[1, -1], [1, -1]]\n",
    "\n",
    "a = Agent2(valences)\n",
    "\n",
    "e = Environment1()\n",
    "# e = Environment2()\n",
    "\n",
    "monde1(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "valences = [[-1, 1], [-1, 1]]\n",
    "e = Environment1()\n",
    "\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 1, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valences = [[-1, 1], [-1, 1]]\n",
    "# valences = [[1, -1], [1, -1]]\n",
    "\n",
    "a = Agent2(valences)\n",
    "\n",
    "# e = Environment1()\n",
    "e = Environment2()\n",
    "\n",
    "monde1(a,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "valences = [[-1, 1], [-1, 1]]\n",
    "e = Environment2()\n",
    "\n",
    "Action: 0, Anticipation: 0, Outcome: 1, Satisfaction: (anticipation: False, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: -1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "\n",
    "L'agent2 se comporte de la même manière que l'agent1 en ce qui concerne l'anticipation des outcomes de ses actions.  \n",
    "En plus de cela, l'agent cherche à maximiser sa valence pour déterminer quelle action effectuer.  \n",
    "\n",
    "1) Initialement, l'agent effectue la seule action qu'il connaît peu importe la valence associée, jusqu'à l'ennui.  \n",
    "2) Ensuite, l'agent passe à l'autre action possible.\n",
    "3) a) Si cette action est associée à une valence positive, il continue jusqu'à l'ennui.  \n",
    "b) Sinon il rechange d'action pour une meilleure valence.\n",
    "4) Il répète ensuite 4 fois les actions avec une meilleure valence et 1 fois les actions avec une moins bonne valence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les comportements des agents sont bien associés à de l'IA développementale car les agents commencent par effectuer une action dans leur environnement. Ce dernier répond par un outcome que l'agent enregistre dans sa mémoire ce qui lui permet d'apprendre à prédire ses prochaines interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'agent qui pilotait une tortue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtlepy_enacter import TurtlePyEnacter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent3:\n",
    "    def __init__(self, valence_table, bored_level=4):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self.valence_table = valence_table\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.nb_actions = 3\n",
    "        \n",
    "        self.count_good_pred = 0\n",
    "        self.bored_level = bored_level\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) +\n",
    "                  \", valence: \" + str(self.valence_table[self._action][outcome]) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            if self.anticipated_outcome == outcome:\n",
    "                self.count_good_pred += 1\n",
    "        \n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = self.prev_action\n",
    "        \n",
    "        best_valence = -1\n",
    "        for a,o in self.prev_outcomes.items():\n",
    "            valence = self.valence_table[a][o]\n",
    "            if valence > best_valence:\n",
    "                best_valence = valence\n",
    "                self._action = a      \n",
    "        if self._action != self.prev_action:\n",
    "            self.count_good_pred = 0\n",
    "        \n",
    "        if self.count_good_pred >= self.bored_level:\n",
    "            self._action = (self.prev_action+1) % self.nb_actions\n",
    "            self.count_good_pred = 0\n",
    "            \n",
    "        self.prev_action = self._action\n",
    "        \n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self.anticipated_outcome = 0\n",
    "        if self._action in self.prev_outcomes:\n",
    "            self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "        return self._action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TurtlePyEnacter()\n",
    "\n",
    "# valences = [[1,-1], [1,-1], [1,-1]]\n",
    "valences = [[-1,-1], [1,-1], [1,-1]]\n",
    "# valences = [[1,-1], [-1,-1], [-1,-1]]\n",
    "\n",
    "agent3 = Agent3(valences, 4)\n",
    "outcome = 0\n",
    "\n",
    "for i in range(500):\n",
    "    action = agent3.action(outcome)\n",
    "    outcome = x.outcome(action)\n",
    "    \n",
    "x.screen.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Agent3 avec bored_level=1](img/agent3_bored_lvl_1.png)\n",
    "*bored_level = 1*\n",
    "![Agent3 avec bored_level=4](img/agent3_bored_lvl_4.png)\n",
    "*bored_level = 4*\n",
    "![Agent3 avec bored_level=10](img/agent3_bored_lvl_10.png)\n",
    "*bored_level = 10*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "\n",
    "**En ne modifiant que le bored_level**  \n",
    "La première chose que l'on remarque est que l'agent finit par se coincer dans une succession d'actions se répétant en boucle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'agent qui adaptait ses actions au contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources import Interaction\n",
    "\n",
    "class Agent4:\n",
    "    def __init__(self, valence_table, bored_level=4):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self.valence_table = valence_table\n",
    "        self._action = None\n",
    "        self.anticipated_outcome = None\n",
    "        \n",
    "        self.nb_actions = len(valence_table)\n",
    "        \n",
    "        self.interactions = {}\n",
    "        self.interaction_tm1 = None\n",
    "        self.interaction_tm2 = None\n",
    "        \n",
    "        self.count_bored = 0\n",
    "        self.bored_level = bored_level\n",
    "        self.prev_outcomes = {}\n",
    "        self.prev_action = 0\n",
    "        \n",
    "        # Réinitialisation des interactions enregistrées\n",
    "        Interaction.interaction_list = []\n",
    "\n",
    "    def action(self, outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(\"Action: \" + str(self._action) +\n",
    "                  \", Anticipation: \" + str(self.anticipated_outcome) +\n",
    "                  \", Outcome: \" + str(outcome) +\n",
    "                  \", Satisfaction: (anticipation: \" + str(self.anticipated_outcome == outcome) +\n",
    "                  \", valence: \" + str(self.valence_table[self._action][outcome]) + \")\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        \n",
    "        # Mémorisation outcome\n",
    "        self.interaction_tm2 = self.interaction_tm1\n",
    "        \n",
    "        if self._action is not None:\n",
    "            self.prev_outcomes[self._action] = outcome\n",
    "            self.interaction_tm1 = Interaction.create_or_retrieve(self._action, outcome, self.valence_table[self._action][outcome])\n",
    "        \n",
    "        if self.interaction_tm2 is not None:\n",
    "            self.interactions[self.interaction_tm2] = self.interaction_tm1\n",
    "        \n",
    "        # Decision and anticipation\n",
    "        self._action = 0\n",
    "        self.anticipated_outcome = None\n",
    "        next_iteration = None\n",
    "        \n",
    "        # Prédiction si cela est possible\n",
    "        if self.interaction_tm1 is not None and self.interaction_tm1 in self.interactions:\n",
    "            next_iteration = self.interactions[self.interaction_tm1]\n",
    "            \n",
    "        if next_iteration is not None:\n",
    "            next_outcome = next_iteration.outcome\n",
    "            # Si l'action prédite à une valence supposée > 0 et que l'agent ne s'ennui pas\n",
    "            if next_iteration.valence > 0 and self.count_bored < (self.bored_level-1):\n",
    "                self._action = next_iteration.action\n",
    "                self.anticipated_outcome = next_outcome\n",
    "            else:\n",
    "                # Sinon on essaye une autre action\n",
    "                self._action = (next_iteration.action + 1) % self.nb_actions\n",
    "                # On essaye de prédire l'outcome si possible\n",
    "                if self._action in self.prev_outcomes:\n",
    "                    self.anticipated_outcome = self.prev_outcomes[self._action]\n",
    "            \n",
    "            # Ennui\n",
    "            if self._action == self.prev_action and self.anticipated_outcome == outcome:\n",
    "                self.count_bored += 1\n",
    "            else:\n",
    "                self.count_bored = 0\n",
    "        \n",
    "        self.prev_action = self._action\n",
    "        return self._action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valences = [[-1, 1], [-1, 1]]\n",
    "# valences = [[1, -1], [1, -1]]\n",
    "\n",
    "a4 = Agent4(valences)\n",
    "\n",
    "# e = Environment1()\n",
    "# e = Environment2()\n",
    "e = Environment3()\n",
    "\n",
    "monde1(a4,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "valences = [[-1, 1], [-1, 1]]\n",
    "e = Environment3()\n",
    "\n",
    "Action: 0, Anticipation: None, Outcome: 0, Satisfaction: (anticipation: False, valence: -1)\n",
    "Action: 0, Anticipation: None, Outcome: 0, Satisfaction: (anticipation: False, valence: -1)\n",
    "Action: 1, Anticipation: None, Outcome: 1, Satisfaction: (anticipation: False, valence: 1)\n",
    "Action: 0, Anticipation: None, Outcome: 1, Satisfaction: (anticipation: False, valence: 1)\n",
    "Action: 0, Anticipation: None, Outcome: 0, Satisfaction: (anticipation: False, valence: -1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 0, Anticipation: 1, Outcome: 1, Satisfaction: (anticipation: True, valence: 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trutle\n",
    "\n",
    "x = TurtlePyEnacter()\n",
    "\n",
    "# valences = [[1,-1], [1,-1], [1,-1]]\n",
    "valences = [[-1,-1], [1,-1], [1,-1]]\n",
    "# valences = [[1,-1], [-1,-1], [-1,-1]]\n",
    "\n",
    "a4 = Agent4(valences, 4)\n",
    "outcome = 0\n",
    "\n",
    "for i in range(200):\n",
    "    action = a4.action(outcome)\n",
    "    outcome = x.outcome(action)\n",
    "    \n",
    "x.screen.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "valences = [[-1,-1], [1,-1], [1,-1]]\n",
    "\n",
    "Action: 0, Anticipation: None, Outcome: 0, Satisfaction: (anticipation: False, valence: -1)\n",
    "Action: 0, Anticipation: None, Outcome: 0, Satisfaction: (anticipation: False, valence: -1)\n",
    "Action: 1, Anticipation: None, Outcome: 0, Satisfaction: (anticipation: False, valence: 1)\n",
    "Action: 0, Anticipation: None, Outcome: 0, Satisfaction: (anticipation: False, valence: -1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 2, Anticipation: None, Outcome: 0, Satisfaction: (anticipation: False, valence: 1)\n",
    "Action: 0, Anticipation: None, Outcome: 0, Satisfaction: (anticipation: False, valence: -1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 2, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 2, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 2, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 2, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
    "Action: 1, Anticipation: 0, Outcome: 0, Satisfaction: (anticipation: True, valence: 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des résultats\n",
    "\n",
    "On observe que l'agent est bien capable de prédire et d'exécuter des actions qui lui apporteront une valence positive. Par ailleurs, suite à des expériences avec l'environnement TurtlePy, nous observons que lorsque l'agent touche les bords, générant une valence négative, il n'arrive pas à s'en sortir, peut être par manque de profondeur d'exploration des possibilités pour retrouver une valence positive.\n",
    "\n",
    "![agent4 au bout de 200 iterations](img/agent4_200iterations.png)\n",
    "*Agent4 au bout de 200 itérations dans l'environnement TurtlePy*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
